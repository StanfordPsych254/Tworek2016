---
title: "Replication of 'Why Do People Tend to Infer “Ought” From “Is”? The Role of Biases in Explanation' by Tworek & Cimpian (2016, Psychological Science)"
author: "Sara Altman skaltman@stanford.edu"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

##Introduction

Study 2 of Tworek and Cimpian (2016) investigated if inherence bias is positively related to subjects' ought inferences when reasoning about typical behaviors. Inherence bias is defined as a preference for explanations that appeal to intrinsic qualities (i.e., color) over explanations that appeal to extrinsic qualities (i.e., historical context). To measure inherence bias, subjects read fifteen extrinsic explanations and fifteen intrinsic explanations and recorded their endorsement of each. Ought inferences were measured by showing subjects twelve statements about different behaviors. Six of these behaviors were considered typical, and six were atypical. Subjects then answered two questions about these behaviors, one of which asked them how "right" or "wrong" the behavior was, and one of which asked them if people should perform the behavior. Subjects' education level, conservatism, and answers to a short Cognitive Reflection Test were also recorded as control measures. 

##Methods

A sample of 130 participants will be recruited from Amazon Mechanical Turk. Subjects will be asked to record their endorsements of 15 pairs of explanations of different phenomena, to answer 12 pairs of questions about twelve different human behaviors, and to answer three short CRT questions. They will be asked about their education level and political leaning at the end of the survey. 

To view the experiment, click here: http://web.stanford.edu/~skaltman/study2.html. 

###Power Analysis

The original study (n = 112) found that the interaction between the measure of inherence bias and behavior typicality was a significant predictor of the measure of ought inferences ($b$ = 2.44, 95% CI = [.78, 4.10]). The original study used a linear mixed-effects model. I estimated the effect size by calculating Cohen's $d_z$ for the difference between the coefficient and 0 ($d_z = 0.27$). The original power to detect this effect was .81. To achieve 80% power, a sample size of 110 is needed. To achieve 90% power, a sample size of 147 is needed. To achieve 95% power, a sample size of 181 is needed. 

###Planned Sample

The original study tested a total of 168 participants and excluded 27 (16% of all participants) because they were either outside the US, failed 2 or more of the attention checks, or because they indicated during the debriefing that they were not paying attention. 

If I plan to exclude the same number of participants, I would need to run 130 participants. 

###Materials

All materials are here: https://osf.io/4kanr/.

###Procedure	

The procedure from the original paper was followed. The text from the original paper is as follows:

"Procedure. Participants were tested online via Mechan- ical Turk using Qualtrics software. The ought measure, the measure of inherence bias, and the CRT were pre- sented in random order. Item order was randomized for all scales. The measures of participants’ education and conservatism were administered with other demographic questions at the end of the survey."

###Analysis Plan

The analysis plan from the original paper was followed. The following is quoted from the original paper:

"Analytic strategy. Because we manipulated behavior typicality within subjects, we used a multilevel model to analyze our data. The model included cross-classified random effects (specifically, intercepts) for subjects and items. Participants’ ought inferences, calculated as the average of their responses to the two ought questions on each trial, served as the dependent variable. The model included as independent variables the typicality of each stimulus behavior (0 = atypical, 1 = typical), participants’ scores on the measure of inherence bias, and the three control measures (i.e., CRT, education, and conserva- tism). The model also included the two-way interactions between behavior typicality and each of the latter four variables. We hypothesized a positive relationship between participants’ inherence bias and their ought inferences for typical—but not atypical—behaviors. Thus, our main prediction was of a significant two-way interaction between the measure of inherence bias and behavior typicality. Including the other two-way interac- tions (with CRT, education, and conservatism) in the model enabled us to explore whether the relationships between these control variables and ought inferences also differed for typical and atypical behaviors. Adjusting for these potential relationships was a conservative anal- ysis strategy; in alternative models that did not include these interactions, the predicted relationship was esti- mated to be larger in magnitude.
For ease of interpretation, we present unstandardized coefficients below. Given the coding of the behavior- typicality variable, the first-order coefficients for the measure of inherence bias, CRT, education, and conser- vatism in this model are simply the slopes of the rela- tionships between these variables and ought inferences for atypical behaviors. Moreover, the slopes for typical behaviors can easily be calculated by adding each first- order coefficient to the coefficient for the corresponding two-way interaction.Can also quote directly, though it is less often spelled out effectively for an analysis strategy section.  The key is to report an analysis strategy that is as close to the original - data cleaning rules, data exclusion rules, covariates, etc. - as possible."

The key analysis was a linear mixed-effects model. 

###Differences from Original Study


### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.


##Results


### Data preparation

Data preparation following the analysis plan.
```{r, message = FALSE, include = FALSE}
library(jsonlite)
library(lme4)
library(nlme)
library(RJSONIO)
library(modelr)
library(readxl)
library(stringr)
library(tidyverse)

#params
attention_check_inherence <- "Intelligent organisms on Earth fully pay attention when taking surveys because of something about intelligent organisms or about taking surveys—maybe paying attention allows intelligent organisms to contribute to research productively. For this item can you please choose choice five?"
attention_check_ought <- "For this question, please slide all the range sliders all the way to the left to indicate that you are paying attention"
typicals <- c("Consider that children typically address their teachers with “Ms.,” “Mrs.,” or “Mr.”",
"Think about how people often celebrate their birthdays with other people.",
"Think about how people often go watch a movie when they go on dates.",
"Think about how people typically give roses as gifts on Valentine\'s Day.",
"Think about how doctors usually wear white coats.",
"Think about how men and women typically have separate public bathrooms.",
"Think about how a lot of professionals wear dark-colored clothing.",
"Consider that people typically stand when the national anthem is played.",
"Consider that people often pay money to watch others play sports.",
"Consider that people generally shake hands when they first meet.",
"Consider that most men wear their hair short.",
"Consider that couples typically live in a different house than their relatives.")
```
	
####Import data

```{r, include = FALSE}
#FOR JSON OBJECTS
# #gets all json files in the folder
# #folder will need to be changed once the experiment is no longer in the sandbox
# json_files <- list.files("~/GitHub/Psych254/Tworek2016/production-results/")
# root_data_path <- "~/GitHub/Psych254/Tworek2016/production-results/"
```

```{r, message = FALSE}
directory <- "~/GitHub/Psych254/Tworek2016/sandbox-results/csvs/"
csv_files <- dir(directory, pattern = ".csv")
```

####Create data frame and tidy data
```{r, include = FALSE, message = FALSE}
data <- tibble()

#Function extracts data from a JSON object and converts to a data frame
get_data_from_json <- function(ID, data) {
  #d <- jsonlite::fromJSON(txt = data_path)
  
  #gathers prompts and answers for ought questions into a tibble
  ought_data <-
    tibble(
      id = ID,
      prompt = c(data$prompts_ought1, data$prompts_ought2, data$prompts_ought3,
                          data$prompts_ought4, data$prompts_ought5, 
                          data$prompts_ought6, 
                          data$prompts_ought7, data$prompts_ought8, 
                          data$prompts_ought9, 
                          data$prompts_ought10, data$prompts_ought11, 
                          data$prompts_ought12, 
                          data$prompts_ought13), 
      prompt_type = "ought",
      should = as.integer(data$should),
      rightwrong = as.integer(data$rightwrong),
      education = as.integer(data$education), #fix so isn't redundant
      political = as.integer(data$political),
      age = data$age,
      gender = data$gender,
      race = data$ethnicity,
      comments = data$comments
      ) %>% 
    group_by(prompt) %>% 
    mutate(average = mean(c(should, rightwrong), na.rm = TRUE)) %>% 
    ungroup() %>% 
    gather(key = measure, value = value, should, rightwrong, average) 
  
  #gathers prompts and answers for inherence questions into a tibble
  inherence_data <-
    tibble(
      id = ID,
      prompt = c(data$prompts_inherence1, data$prompts_inherence2, 
                              data$prompts_inherence3, data$prompts_inherence4, 
                              data$prompts_inherence5, data$prompts_inherence6, 
                              data$prompts_inherence7, data$prompts_inherence8, 
                              data$prompts_inherence9, data$prompts_inherence10, 
                              data$prompts_inherence11, data$prompts_inherence12, 
                              data$prompts_inherence13, data$prompts_inherence14,
                              data$prompts_inherence15, data$prompts_inherence16),
      prompt_type = "inherence",
      intrinsic = as.integer(data$intrinsic),
      extrinsic = as.integer(data$extrinsic),
      education = as.integer(data$education),
      political = as.integer(data$political),
      age = data$age,
      gender = data$gender,
      race = data$ethnicity,
      comments = data$comments
    ) %>% 
    gather(key = measure, value = value, intrinsic, extrinsic)
  
  #gathers prompts and answers for crt questions into a tibble
  crt_data <-
    tibble(
      id = ID,
      prompt = c(data$prompts_crt1, data$prompts_crt2, 
                       data$prompts_crt3),
      prompt_type = "crt",
      crt = data$crt, 
      education = as.integer(data$education),
      political = as.integer(data$political),
      age = data$age,
      gender = data$gender,
      race = data$ethnicity,
      comments = data$comments
    ) %>% 
    gather(key = measure, value = value, crt) %>% 
    mutate(value_chr = str_extract(value, "\\d+([[:punct:]]*\\d+)*"), #extract number out of text box entry 
           value = as.double(str_replace(value_chr, ",", "."))) %>% #replace commas with decimal points 
    select(-value_chr)
  
  
  #binds all tibbles together into a single tidy data frame
  return(bind_rows(crt_data, inherence_data, ought_data))
}
```

```{r, include = FALSE}
#for json
#for extracting data from json objects and not from csv
#loops over all json objects and extracts data
# for (i in 1:length(json_files)) {
#   data_path <- str_c(root_data_path, json_files[i])
#   data_participant <- get_data_from_json(data_path)
#   data <- bind_rows(data, data_participant)
# }
```

The following loops through all the csv files containing the data and creates the data frame:
```{r, message = FALSE}
for (file in csv_files) {
  jd <- read_csv(paste(paste(directory, file, sep = "")))
  jd <-
    jd %>% 
    mutate(data = iconv(jd$Answer.data, "latin1", "ASCII", sub = "")) %>% 
    select(-Answer.data)

  answers <- jd$data
  file.create("answers.json")
  fileConn <- file("answers.json")
  writeLines(answers, fileConn)
  close(fileConn)
  answers <- fromJSON("answers.json")
  data_participant <- get_data_from_json(jd$WorkerId, answers)
  data <- bind_rows(data, data_participant)
}
```

#### Data exclusion/filtering

The original paper excluded participants that failed two or more of the four attention checks.
```{r, include = FALSE}
to_exclude_inherence_extrinsic <-
  data %>% 
  filter(prompt == attention_check_inherence,
         measure == "extrinsic",
         value != 3) %>% 
  select(id)

#find id's of subjects who failed the inherence bias-extrinsic question attention check
to_exclude_inherence_intrinsic <-
  data %>% 
  filter(prompt == attention_check_inherence,
         measure == "intrinsic",
         value != 5) %>% 
  select(id)

#find id's of subjects who failed the ought inference-should question attention check
to_exclude_ought_should <-
  data %>% 
  filter(prompt == attention_check_ought,
         measure == "should",
         value != 0) %>% 
  select(id)

#find id's of subjects who failed the ought inference-right/wrong question attention check
to_exclude_ought_rightwrong <-
  data %>% 
  filter(prompt == attention_check_ought,
         measure == "rightwrong",
         value != 0) %>% 
  select(id)
```


```{r}
#create list of id's of participants who failed 2 or more attention checks 
to_exclude <- 
  bind_rows(to_exclude_inherence_extrinsic, 
                to_exclude_inherence_intrinsic, 
                to_exclude_ought_should, 
                to_exclude_ought_rightwrong) %>% 
  group_by(id) %>% 
  filter(n() >= 2) %>% 
  .$id

#filter out participants that we want to exclude
data <-
  data %>% 
  filter(!(id %in% to_exclude))
```

#### Prepare data for analysis

The following creates a tibble with the measure of inherence bias for each participant. The original paper defined inherence bias as the difference between a participant's endorsement of intrinsic explanations and his/her endorsement of extrinsic explanations. 

```{r}
inherence_scores <-
  data %>% 
  group_by(id) %>% 
  filter(prompt_type == "inherence") %>% 
  tidyr::spread(measure, value) %>% 
  summarise(inherence_bias = mean(intrinsic) - mean(extrinsic))
```

The following creates a tibble of the CRT scores for each participant:
```{r}
#score the nurse CRT question
grade_nurse_crt <-
  data %>% 
  filter(prompt_type == "crt",
         str_detect(prompt, "nurses")) %>% 
  mutate(nurse_score = ifelse(near(value, 2.00), 1, 0))

#score the salad CRT question
grade_salad_crt <-
  data %>% 
  filter(prompt_type == "crt",
         str_detect(prompt, "salad")) %>% 
  mutate(salad_score = ifelse(near(value, 2.25), 1, 0))

#score the sally CRT question
grade_sally_crt <-
  data %>% 
  filter(prompt_type == "crt",
         str_detect(prompt, "Sally")) %>% 
  mutate(sally_score = ifelse(near(value, 5.00), 1, 0))

#join all CRT scores
crt_scores <-
  grade_nurse_crt %>% 
  left_join(grade_salad_crt, by = "id") %>% 
  left_join(grade_sally_crt, by = "id") %>% 
  group_by(id) %>% 
  summarise(crt_score = sum(sally_score, nurse_score, salad_score)/3) 
```

The following removes the attention checks prompts so that they are not included in the analysis:
```{r}
data_filtered <-
  data %>%
  filter(prompt != attention_check_inherence,
         prompt != attention_check_ought) 
```

Now, we can join the data with the inherence bias measures and CRT scores:
```{r}
data_scores <-
  data_filtered %>%
  group_by(id) %>% 
  left_join(inherence_scores, by = "id") %>% 
  left_join(crt_scores, by = "id") %>% 
  filter(measure == "average") %>% 
  rename(ought_inference = value)
```

Finally, we need to add in the binary typicality measure for each prompt:
```{r}
data_final <-
  data_scores %>% 
  mutate(typicality = as.integer(ifelse(prompt %in% typicals, 1, 0)))
```

The data is now ready for analysis. 

### Confirmatory analysis

As detailed in the original paper, we fit a linear mixed-effects model with subjects and items as random intercept effects. 
```{r, eval=FALSE}
#Currently p > n so we can't fit the full model-when more data is collected it will fit the full model
fit <- lmer(ought_inference ~ inherence_bias*typicality + 
              crt_score*typicality + education*typicality + 
              political*typicality + (1 | id) + (1 | prompt), 
            data = data_final)

data.frame(coef(summary(fit))) %>% 
  mutate(p = 2 * (1 - pnorm(abs(coefs$t.value)))) %>% 
  knitr::kable()
```

Original plot:

![Original plot](http://web.stanford.edu/~skaltman/original_plot.png)

I recreated the plot with the new data:

```{r, eval=FALSE}
mean_ib <- mean(data_final$inherence_bias, na.rm = TRUE)
sd_ib <- sd(data_final$inherence_bias, na.rm = TRUE)

#calculates if the inherence bias is one sd over the mean, one sd under the mean, or neither
find_ib_group <- function(inherence_bias) {
  if (inherence_bias <= mean_ib - sd_ib) {return("below")}
  if (inherence_bias >= mean_ib + sd_ib) {return("above")}
  else {return("NA")}
}

#tibble grouped by inherence bias level
by_ib_group <-
  data_final %>% 
  mutate(inherence_bias_group = 
           map_chr(inherence_bias, find_ib_group)) %>% 
  filter(inherence_bias_group != "NA") %>% 
  group_by(inherence_bias_group, typicality) %>% 
  summarise(mean_ought_inference = 
              mean(ought_inference, na.rm = TRUE),
            se = sd(ought_inference, na.rm = TRUE)/sqrt(n()))

#recreate plot from original paper
by_ib_group %>% 
  ggplot(aes(inherence_bias_group, mean_ought_inference, linetype = as.factor(typicality))) +
  geom_point() +
  geom_errorbar(aes(ymin = mean_ought_inference - se, 
                    ymax = mean_ought_inference + se),
                size = .25, width = .05) +
  geom_line(aes(group = typicality)) +
  labs(y = "Ought Inference (0-100)",
       x = NULL) +
  scale_x_discrete(labels = c("Weak Inherence Bias\n(1 SD Below the\nMean)", "Strong Inherence Bias\n(1 SD Above the\nMean)")) +
  scale_y_continuous(breaks = seq(0, 100, 20)) +
  coord_cartesian(ylim = c(0, 100)) + #this will change--it's too hard to see the errorbars otherwise
  #coord_cartesian(ylim = c(0, 100)) +
  scale_linetype_discrete(name = NULL, labels = c("Atypical behaviors", "Typical behaviors")) +
  theme_minimal() +
  theme(panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank())
```

###Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
